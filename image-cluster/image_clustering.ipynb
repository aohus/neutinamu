{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aohus/.pyenv/versions/3.11.8/envs/local-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import ops\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# 이미지 경로 설정\n",
    "\n",
    "IMAGE_DIR = \"/Users/aohus/Workspaces/github/image-cluster/2차\"\n",
    "# 이미지 목록 로딩\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR, fname)\n",
    "    for fname in os.listdir(IMAGE_DIR)\n",
    "    if fname.lower().endswith((\"png\", \"jpg\", \"jpeg\"))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aohus/.pyenv/versions/3.11.8/envs/local-venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 185 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/resnet_vd/keras/resnet_vd_200_imagenet/2/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 910/910 [00:00<00:00, 2.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/resnet_vd/keras/resnet_vd_200_imagenet/2/download/model.weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280M/280M [00:25<00:00, 11.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import keras_hub\n",
    "\n",
    "# 분류 모델 로딩\n",
    "model = keras_hub.models.DeepLabV3ImageSegmenter.from_preset(\n",
    "    \"deeplab_v3_plus_resnet50_pascalvoc\"\n",
    ")\n",
    "image_converter = keras_hub.layers.DeepLabV3ImageConverter(\n",
    "    image_size=(512, 512),\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "preprocessor = keras_hub.models.DeepLabV3ImageSegmenterPreprocessor(image_converter)\n",
    "\n",
    "\n",
    "# CNN 모델 로딩\n",
    "effnet_model = EfficientNetB4(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "resnet_model = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "effnetv2_model = keras_hub.models.Backbone.from_preset(\n",
    "    \"efficientnet_b5_sw_ft_imagenet\",\n",
    ")\n",
    "resnetv2_model = keras_hub.models.Backbone.from_preset(\n",
    "    \"resnet_vd_200_imagenet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_human(image_path):\n",
    "    image = keras.utils.load_img(image_path)\n",
    "    image = np.array(image)\n",
    "\n",
    "\n",
    "    image = preprocessor(image)\n",
    "    image = keras.ops.expand_dims(np.array(image), axis=0)\n",
    "    seg = ops.argmax(model.predict(image, verbose=0), axis=-1)\n",
    "    human_mask = ops.expand_dims(seg, axis=-1)[0] == 15  # numpy로 마스크 생성\n",
    "\n",
    "    human_mask_expanded = tf.expand_dims(human_mask, axis=0)\n",
    "    human_mask_expanded = tf.broadcast_to(human_mask, tf.shape(image))\n",
    "\n",
    "    # 마스크 위치를 0으로 만든 새 텐서 생성\n",
    "    masked_image = tf.where(human_mask_expanded, tf.zeros_like(image), image)\n",
    "    return masked_image\n",
    "\n",
    "# 수정된 함수 (마스킹된 이미지를 특징 벡터로 변환)\n",
    "def batch_extract_background_features(img_paths, cnn_model, batch_size=16):\n",
    "    features = []\n",
    "\n",
    "    for i in tqdm(range(0, len(img_paths), batch_size)):\n",
    "        batch_paths = img_paths[i : i + batch_size]\n",
    "\n",
    "        batch_imgs = []\n",
    "        for p in batch_paths:\n",
    "            preds = masking_human(p)  # 사람 마스킹 이미지 반환 (Tensor 또는 np.array)\n",
    "            preds = np.squeeze(preds)\n",
    "            # preds가 TensorFlow tensor면 numpy로 변환\n",
    "            if hasattr(preds, 'numpy'):\n",
    "                preds = preds.numpy()\n",
    "\n",
    "            # PIL 이미지 변환 및 크기 조정 (224x224)\n",
    "            img_pil = Image.fromarray(preds.astype('uint8')).resize((224, 224))\n",
    "            img_array = img_to_array(img_pil)\n",
    "\n",
    "            batch_imgs.append(img_array)\n",
    "\n",
    "        # CNN 입력 형태로 변환 (전처리)\n",
    "        batch_imgs_array = preprocess_input(np.array(batch_imgs))\n",
    "\n",
    "        # EfficientNet으로 특징 벡터 추출\n",
    "        batch_features = cnn_model.predict(batch_imgs_array, verbose=0)\n",
    "        if len(batch_features.shape) != 2:\n",
    "            batch_features = batch_features.reshape(batch_features.shape[0], -1)  # (batch_size, feature_dim)\n",
    "        features.extend(batch_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# 배치로 특징 추출 (속도 최적화)\n",
    "def batch_extract_features(img_paths, cnn_model, batch_size=32):\n",
    "    features = []\n",
    "    num_images = len(img_paths)\n",
    "\n",
    "    for i in tqdm(range(0, num_images, batch_size)):\n",
    "        batch_paths = img_paths[i:i+batch_size]\n",
    "        \n",
    "        # 배치 단위로 이미지 로드 및 전처리\n",
    "        imgs = [image.img_to_array(image.load_img(p, target_size=(224,224))) for p in batch_paths]\n",
    "        imgs_array = np.array(imgs)\n",
    "        imgs_array = preprocess_input(imgs_array)\n",
    "\n",
    "        # 배치 예측 (한 번에 처리)\n",
    "        batch_features = cnn_model.predict(imgs_array, verbose=0)\n",
    "        if len(batch_features.shape) != 2:\n",
    "            batch_features = batch_features.reshape(batch_features.shape[0], -1)  # (batch_size, feature_dim)\n",
    "        features.extend(batch_features)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "import cv2\n",
    "# 녹색 및 갈색 마스크 생성 함수\n",
    "def extract_green_brown(img_array):\n",
    "    # HSV로 변환\n",
    "    hsv_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # 녹색 범위 설정 (HSV)\n",
    "    green_lower = np.array([35, 40, 40])\n",
    "    green_upper = np.array([85, 255, 255])\n",
    "    green_mask = cv2.inRange(hsv_img, green_lower, green_upper)\n",
    "\n",
    "    # 갈색 범위 설정 (HSV)\n",
    "    brown_lower = np.array([10, 50, 50])\n",
    "    brown_upper = np.array([30, 255, 200])\n",
    "    brown_mask = cv2.inRange(hsv_img, brown_lower, brown_upper)\n",
    "\n",
    "    # 녹색과 갈색 마스크 결합\n",
    "    combined_mask = cv2.bitwise_or(green_mask, brown_mask)\n",
    "\n",
    "    # 원본 이미지에 마스크 적용\n",
    "    masked_img = cv2.bitwise_and(img_array, img_array, mask=combined_mask)\n",
    "\n",
    "    return masked_img\n",
    "\n",
    "# 특징 추출 함수\n",
    "def batch_extract_colored_features(img_paths, cnn_model, batch_size=32):\n",
    "\n",
    "    features = []\n",
    "    num_images = len(img_paths)\n",
    "\n",
    "    for i in tqdm(range(0, num_images, batch_size)):\n",
    "        batch_paths = img_paths[i:i+batch_size]\n",
    "        \n",
    "        # 배치 단위로 이미지 로드 및 전처리\n",
    "        imgs = [ Image.open(p).convert('RGB').resize((224,224)) for p in batch_paths ]\n",
    "        gb_imgs = [ extract_green_brown(np.array(img)) for img in imgs ]\n",
    "        imgs_array = np.array(gb_imgs)\n",
    "        imgs_array = preprocess_input(imgs_array)\n",
    "\n",
    "        # 배치 예측 (한 번에 처리)\n",
    "        batch_features = cnn_model.predict(imgs_array, verbose=0)\n",
    "        if len(batch_features.shape) != 2:\n",
    "            batch_features = batch_features.reshape(batch_features.shape[0], -1)  # (batch_size, feature_dim)\n",
    "        features.extend(batch_features)\n",
    "\n",
    "    \n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특징 추출 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:29<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특징 추출 완료: (244, 100352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 특징 추출 실행\n",
    "print(\"특징 추출 중...\")\n",
    "\n",
    "# batch_extract_background_features\n",
    "# batch_extract_features\n",
    "# batch_extract_colored_features\n",
    "\n",
    "cnn_model = resnetv2_model\n",
    "extract_features = batch_extract_features\n",
    "features = extract_features(image_paths, cnn_model)\n",
    "\n",
    "\n",
    "print(\"특징 추출 완료:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터별 이미지 저장 중...\n",
      "분류가 완료되었습니다. 결과는 '/Users/aohus/Workspaces/github/image-cluster/2차_res_net_backbone_9_batch_extract_features_kmeans'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = IMAGE_DIR + f\"_{cnn_model.name}_{extract_features.__name__}_kmeans\"\n",
    "\n",
    "# 클러스터링 (KMeans, GPS 정보 포함)\n",
    "num_clusters = 60\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(features)\n",
    "\n",
    "# 클러스터링 결과 저장\n",
    "print(\"클러스터별 이미지 저장 중...\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    cluster_folder = os.path.join(OUTPUT_DIR, f\"cluster_{label}\")\n",
    "    os.makedirs(cluster_folder, exist_ok=True)\n",
    "    shutil.copy(image_paths[idx], cluster_folder)\n",
    "\n",
    "print(f\"분류가 완료되었습니다. 결과는 '{OUTPUT_DIR}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# 유사도 기반 그룹핑 함수\n",
    "def group_similar_images(features, image_paths, threshold):\n",
    "    similarity_matrix = cosine_similarity(features)\n",
    "    used = np.zeros(len(features), dtype=bool)\n",
    "    groups = []\n",
    "    \n",
    "    for idx in range(len(features)):\n",
    "        if not used[idx]:\n",
    "            # 현재 이미지와 유사도 높은 이미지 인덱스 찾기\n",
    "            similar_idxs = np.where(similarity_matrix[idx] >= threshold)[0]\n",
    "            groups.append(similar_idxs)\n",
    "            used[similar_idxs] = True\n",
    "\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비슷한 이미지 그룹 저장 중...\n",
      "이미지 분류 완료, 결과 폴더: '/Users/aohus/Workspaces/github/image-cluster/2차_efficient_net_backbone_batch_extract_features_cosine_similarity'\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = IMAGE_DIR + f\"_{cnn_model.name}_{extract_features.__name__}_cosine_similarity\"\n",
    "\n",
    "# 유사도 기반 그룹 생성\n",
    "groups = group_similar_images(features, image_paths, threshold=0.6)\n",
    "\n",
    "# 그룹별 폴더 저장\n",
    "print(\"비슷한 이미지 그룹 저장 중...\")\n",
    "for group_id, group_idxs in enumerate(groups):\n",
    "    group_folder = os.path.join(OUTPUT_DIR, f\"group_{group_id}\")\n",
    "    os.makedirs(group_folder, exist_ok=True)\n",
    "    for idx in group_idxs:\n",
    "        shutil.copy(image_paths[idx], group_folder)\n",
    "\n",
    "print(f\"이미지 분류 완료, 결과 폴더: '{OUTPUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
